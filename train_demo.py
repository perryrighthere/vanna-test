# #!/usr/bin/env python3
# """
# Demonstration of Using Trained Vanna Model

# This script shows how to use a trained Vanna model for:
# 1. Natural language to SQL generation
# 2. Query execution and result analysis
# 3. Follow-up question generation
# 4. Interactive querying workflow
# 5. Model performance evaluation
# """

# from vanna.openai import OpenAI_Chat
# from vanna.qdrant import Qdrant_VectorStore
# from qdrant_client import QdrantClient
# import httpx
# from openai import OpenAI
# import pandas as pd
# import time
# from typing import List, Tuple, Optional


# def setup_trained_vanna():
#     """Set up connection to already trained Vanna instance"""
#     try:
#         # OpenAI client setup
#         client = OpenAI(
#             base_url="http://aiapi-api.test.seres.cn/v1",
#             api_key="sk-gdKNRLPALlfgJO2B10iv42fJC5NnEpPDw9MbseYCGCSYSgMC",
#             http_client=httpx.Client(verify=False)
#         )
        
#         # Qdrant client setup (should contain trained data)
#         qdrant = QdrantClient(url="http://localhost:6333")
        
#         # Vanna class combining both backends
#         class MyVanna(Qdrant_VectorStore, OpenAI_Chat):
#             def __init__(self, client=None, config=None):
#                 Qdrant_VectorStore.__init__(self, config=config)
#                 OpenAI_Chat.__init__(self, client=client, config=config)
        
#         # Initialize Vanna with trained data
#         vn = MyVanna(client=client, config={'client': qdrant, 'model': 'qwen2.5-instruct'})
        
#         # Connect to MySQL database
#         vn.connect_to_mysql(
#             host='srtest.seres.cn', 
#             dbname='doris_mcp_poc', 
#             user='doris_mcp_u', 
#             password='zjjKtmu2iDlNCnS0', 
#             port=9030
#         )
        
#         print("âœ… Connected to trained Vanna instance")
#         return vn
        
#     except Exception as e:
#         print(f"âŒ Error connecting to trained Vanna: {e}")
#         return None


# def demonstrate_sql_generation(vn):
#     """Demonstrate natural language to SQL generation"""
#     print("\nğŸ”§ SQL GENERATION FROM NATURAL LANGUAGE")
#     print("=" * 50)
    
#     # Test questions covering different query types
#     test_questions = [
#         # Basic counting and filtering
#         "How many employees are there in total?",
#         "How many active employees do we have?",
#         "How many key employees are in the system?",
        
#         # Grouping and aggregation
#         "What is the gender distribution of employees?",
#         "Show me the age distribution by company",
#         "Count employees by marital status",
        
#         # Filtering with Chinese text
#         "Find all employees from é‡åº†å¸‚",
#         "Show me employees from å››å·çœ", 
#         "List all married employees",
        
#         # Complex analytical queries
#         "What is the average age by organization?",
#         "Find the youngest employee in each company",
#         "Show me employees with complete height and weight data",
        
#         # Management and hierarchy
#         "List all management positions (cadre_label = 1)",
#         "Find core position employees who are also key employees",
#         "Show nationality distribution"
#     ]
    
#     sql_results = []
    
#     for i, question in enumerate(test_questions, 1):
#         print(f"\n[{i:2d}] Question: {question}")
#         try:
#             start_time = time.time()
#             sql = vn.generate_sql(question)
#             generation_time = time.time() - start_time
            
#             print(f"     Generated SQL: {sql}")
#             print(f"     Generation time: {generation_time:.2f}s")
            
#             sql_results.append({
#                 'question': question,
#                 'sql': sql,
#                 'generation_time': generation_time,
#                 'status': 'success'
#             })
            
#         except Exception as e:
#             print(f"     âŒ Error: {e}")
#             sql_results.append({
#                 'question': question,
#                 'sql': None,
#                 'generation_time': 0,
#                 'status': 'failed',
#                 'error': str(e)
#             })
    
#     return sql_results


# def demonstrate_query_execution(vn, sql_results: List[dict]):
#     """Demonstrate executing generated SQL and analyzing results"""
#     print("\nğŸ“Š SQL EXECUTION AND RESULT ANALYSIS")
#     print("=" * 50)
    
#     executed_results = []
    
#     for result in sql_results[:8]:  # Execute first 8 successful queries
#         if result['status'] != 'success' or not result['sql']:
#             continue
            
#         print(f"\nğŸ” Executing: {result['question']}")
#         print(f"   SQL: {result['sql']}")
        
#         try:
#             start_time = time.time()
#             df = vn.run_sql(result['sql'])
#             execution_time = time.time() - start_time
            
#             if df is not None and len(df) > 0:
#                 print(f"   âœ… Results: {len(df)} rows returned in {execution_time:.2f}s")
#                 print(f"   Preview:")
#                 print(df.head().to_string(index=False))
                
#                 executed_results.append({
#                     'question': result['question'],
#                     'sql': result['sql'],
#                     'row_count': len(df),
#                     'execution_time': execution_time,
#                     'data': df
#                 })
#             else:
#                 print("   âš ï¸  No results returned")
                
#         except Exception as e:
#             print(f"   âŒ Execution error: {e}")
    
#     return executed_results


# def demonstrate_ask_workflow(vn):
#     """Demonstrate the complete ask() workflow"""
#     print("\nğŸ’¬ COMPLETE ASK() WORKFLOW DEMONSTRATION")
#     print("=" * 50)
    
#     ask_questions = [
#         "How many employees are in each company?",
#         "What's the gender breakdown?",
#         "Show me key employees with their details",
#         "Find employees older than 35"
#     ]
    
#     for question in ask_questions:
#         print(f"\nğŸ¤– Asking: '{question}'")
#         print("-" * 60)
        
#         try:
#             # Use the complete ask() method
#             start_time = time.time()
#             result = vn.ask(
#                 question=question,
#                 print_results=False,  # We'll handle printing ourselves
#                 auto_train=True,      # Automatically learn from successful queries
#                 visualize=False       # Skip visualization for this demo
#             )
#             total_time = time.time() - start_time
            
#             if result:
#                 sql, df, plot = result
#                 print(f"Generated SQL: {sql}")
#                 print(f"Total processing time: {total_time:.2f}s")
                
#                 if df is not None and len(df) > 0:
#                     print(f"Results ({len(df)} rows):")
#                     print(df.to_string(index=False))
                    
#                     # Generate follow-up questions
#                     try:
#                         followup_questions = vn.generate_followup_questions(
#                             question=question,
#                             sql=sql,
#                             df=df,
#                             n_questions=3
#                         )
#                         print(f"\nSuggested follow-up questions:")
#                         for i, followup in enumerate(followup_questions[:3], 1):
#                             if followup.strip():
#                                 print(f"  {i}. {followup.strip()}")
#                     except Exception as e:
#                         print(f"Could not generate follow-up questions: {e}")
#                 else:
#                     print("No results returned")
#             else:
#                 print("âŒ Ask failed - no result returned")
                
#         except Exception as e:
#             print(f"âŒ Error in ask workflow: {e}")


# def demonstrate_training_data_usage(vn):
#     """Show how training data influences query generation"""
#     print("\nğŸ“š TRAINING DATA INFLUENCE DEMONSTRATION")
#     print("=" * 50)
    
#     # Check current training data
#     try:
#         training_data = vn.get_training_data()
#         print(f"Current training data: {len(training_data)} items")
#         if len(training_data) > 0:
#             print("Training data types:")
#             if 'content' in training_data.columns:
#                 for _, row in training_data.head(5).iterrows():
#                     content = str(row['content'])[:100] + "..." if len(str(row['content'])) > 100 else str(row['content'])
#                     print(f"  - {content}")
#     except Exception as e:
#         print(f"Could not retrieve training data: {e}")
    
#     # Test similar question retrieval
#     print(f"\nğŸ” Testing context retrieval for questions:")
#     test_questions = [
#         "Show me employee count by company",
#         "What is the age distribution?", 
#         "Find key employees"
#     ]
    
#     for question in test_questions:
#         print(f"\nQuestion: {question}")
#         try:
#             # Get similar questions from training data
#             similar_questions = vn.get_similar_question_sql(question=question)
#             if len(similar_questions) > 0:
#                 print("Similar training examples found:")
#                 for _, row in similar_questions.head(2).iterrows():
#                     print(f"  - Question: {row.get('question', 'N/A')}")
#                     sql_preview = str(row.get('content', ''))[:80] + "..." if len(str(row.get('content', ''))) > 80 else str(row.get('content', ''))
#                     print(f"    SQL: {sql_preview}")
#             else:
#                 print("  No similar examples found")
#         except Exception as e:
#             print(f"  Error retrieving similar questions: {e}")


# def analyze_model_performance(sql_results: List[dict], executed_results: List[dict]):
#     """Analyze the performance of the trained model"""
#     print("\nğŸ“ˆ MODEL PERFORMANCE ANALYSIS")
#     print("=" * 50)
    
#     total_questions = len(sql_results)
#     successful_generation = len([r for r in sql_results if r['status'] == 'success'])
#     successful_execution = len(executed_results)
    
#     print(f"SQL Generation Performance:")
#     print(f"  Total questions: {total_questions}")
#     print(f"  Successful generations: {successful_generation} ({successful_generation/total_questions*100:.1f}%)")
#     print(f"  Successful executions: {successful_execution}")
    
#     if successful_generation > 0:
#         avg_generation_time = sum(r['generation_time'] for r in sql_results if r['status'] == 'success') / successful_generation
#         print(f"  Average generation time: {avg_generation_time:.2f}s")
    
#     if executed_results:
#         avg_execution_time = sum(r['execution_time'] for r in executed_results) / len(executed_results)
#         total_rows = sum(r['row_count'] for r in executed_results)
#         print(f"  Average execution time: {avg_execution_time:.2f}s")
#         print(f"  Total rows returned: {total_rows}")
    
#     print(f"\nQuery Type Analysis:")
#     query_types = {
#         'Count queries': len([r for r in sql_results if r['status'] == 'success' and 'COUNT' in str(r['sql']).upper()]),
#         'Group by queries': len([r for r in sql_results if r['status'] == 'success' and 'GROUP BY' in str(r['sql']).upper()]),
#         'Filter queries': len([r for r in sql_results if r['status'] == 'success' and 'WHERE' in str(r['sql']).upper()]),
#         'Complex queries': len([r for r in sql_results if r['status'] == 'success' and ('AVG' in str(r['sql']).upper() or 'MIN' in str(r['sql']).upper() or 'MAX' in str(r['sql']).upper())])
#     }
    
#     for query_type, count in query_types.items():
#         print(f"  {query_type}: {count}")


# def interactive_demo():
#     """Interactive demo where user can ask questions"""
#     print("\nğŸ’» INTERACTIVE DEMO")
#     print("=" * 50)
#     print("Enter questions in natural language (type 'quit' to exit)")
#     print("Examples:")
#     print("  - How many employees are there?")
#     print("  - Show me the gender distribution")
#     print("  - Find key employees from é‡åº†å¸‚")
    
#     vn = setup_trained_vanna()
#     if not vn:
#         print("âŒ Could not connect to trained Vanna")
#         return
    
#     while True:
#         try:
#             question = input("\nğŸ¤” Your question: ").strip()
#             if question.lower() in ['quit', 'exit', 'q']:
#                 break
                
#             if not question:
#                 continue
                
#             print(f"\nğŸ¤– Processing: '{question}'")
            
#             # Generate SQL
#             sql = vn.generate_sql(question)
#             print(f"Generated SQL: {sql}")
            
#             # Execute and show results
#             df = vn.run_sql(sql)
#             if df is not None and len(df) > 0:
#                 print(f"\nResults ({len(df)} rows):")
#                 if len(df) > 10:
#                     print(df.head(10).to_string(index=False))
#                     print(f"... and {len(df) - 10} more rows")
#                 else:
#                     print(df.to_string(index=False))
#             else:
#                 print("No results found")
                
#         except KeyboardInterrupt:
#             break
#         except Exception as e:
#             print(f"âŒ Error: {e}")
    
#     print("\nğŸ‘‹ Thanks for trying the interactive demo!")


# if __name__ == "__main__":
#     print("ğŸš€ TRAINED VANNA USAGE DEMONSTRATION")
#     print("=" * 60)
    
#     # Setup connection to trained model
#     vn = setup_trained_vanna()
#     training_data = vn.get_training_data()
#     print(f"Found {len(training_data)} training items")
    
#     if not vn:
#         print("âŒ Cannot proceed without trained Vanna connection")
#         print("Please run investigate_vanna_training.py first to train the model")
#         exit(1)
    
#     try:
#         # 1. Demonstrate SQL generation capabilities
#         sql_results = demonstrate_sql_generation(vn)
        
#         # 2. Execute some queries and analyze results
#         executed_results = demonstrate_query_execution(vn, sql_results)
        
#         # 3. Show complete ask() workflow
#         demonstrate_ask_workflow(vn)
        
#         # 4. Show training data influence
#         demonstrate_training_data_usage(vn)
        
#         # 5. Analyze model performance
#         analyze_model_performance(sql_results, executed_results)
        
#         # 6. Interactive demo (optional - uncomment to try)
#         # interactive_demo()
        
#         print("\n\nâœ¨ DEMONSTRATION COMPLETE!")
#         print("Key takeaways:")
#         print("âœ… Trained Vanna can generate SQL from natural language")
#         print("âœ… Training data significantly improves accuracy")
#         print("âœ… Model handles Chinese text and business context")
#         print("âœ… Complete workflow: question â†’ SQL â†’ results â†’ insights")
#         print("âœ… Auto-training improves model over time")
        
#     except Exception as e:
#         print(f"\nâŒ Demo failed: {e}")
#         print("Make sure you have:")
#         print("  - Run the training script first")
#         print("  - Qdrant server running with training data")
#         print("  - Valid database connection")

#!/usr/bin/env python3
"""
Investigation of Vanna Training Capabilities with Employee Data

This script demonstrates how to train Vanna with different types of data using real connections:
1. DDL statements (schema definitions)
2. Documentation (business context)
3. Question-SQL pairs (examples)
4. Training data management operations
"""

from vanna.openai import OpenAI_Chat
from vanna.qdrant import Qdrant_VectorStore
from qdrant_client import QdrantClient
import httpx
from openai import OpenAI
import pandas as pd


def setup_vanna_connection():
    """Set up Vanna with OpenAI and Qdrant backends"""
    try:
        # OpenAI client setup
        client = OpenAI(
            base_url="https://aiapi.test.seres.cn/v1",
            api_key="sk-KzbWXgnY22sSBtkSNVSOscVH1SVIZCLTgoGzrqnnjKEQflqA",
            http_client=httpx.Client(verify=False)
        )
        
        # Qdrant client setup
        qdrant = QdrantClient(url="http://localhost:6333")
        
        # Vanna class combining both backends
        class MyVanna(Qdrant_VectorStore, OpenAI_Chat):
            def __init__(self, client=None, config=None):
                Qdrant_VectorStore.__init__(self, config=config)
                OpenAI_Chat.__init__(self, client=client, config=config)
        
        # Initialize Vanna
        vn = MyVanna(client=client, config={'client': qdrant, 'model': 'DeepSeek-V3-0324'})
        
        # Connect to MySQL database
        vn.connect_to_mysql(
            host='srtest.seres.cn', 
            dbname='doris_mcp_poc', 
            user='doris_mcp_u', 
            password='zjjKtmu2iDlNCnS0', 
            port=9030
        )
        
        print("âœ… Successfully connected to Vanna with OpenAI + Qdrant + MySQL")
        return vn
        
    except Exception as e:
        print(f"âŒ Error setting up Vanna connection: {e}")
        print("Note: This requires running Qdrant locally and valid API keys")
        return None


def display_training_data_summary(vn):
    """Display current training data in Qdrant"""
    try:
        training_data = vn.get_training_data()
        if len(training_data) > 0:
            print(f"\nğŸ“Š Current training data: {len(training_data)} items")
            print(training_data.head(10))
        else:
            print("\nğŸ“Š No training data found")
        return training_data
    except Exception as e:
        print(f"âŒ Error retrieving training data: {e}")
        return pd.DataFrame()


def demonstrate_employee_data_training(vn):
    """Demonstrate training Vanna with the employee data schema and related information"""
    
    print("ğŸ” INVESTIGATING VANNA TRAINING CAPABILITIES")
    print("=" * 50)
    
    # Show initial training data state
    display_training_data_summary(vn)
    
    print("\n1. TRAINING WITH DDL STATEMENTS")
    print("-" * 30)
    
    # BI dataset table DDL based on the actual schema
    bi_dataset_ddl = """
    CREATE TABLE bi_dataset (
        person_code VARCHAR(65533) NULL,
        position_code VARCHAR(65533) NOT NULL,
        position_name VARCHAR(65533) NOT NULL,
        organization_code VARCHAR(65533) NOT NULL,
        organization_name VARCHAR(65533) NOT NULL,
        empl_status VARCHAR(65533) NOT NULL,
        company_code VARCHAR(65533) NOT NULL,
        company_name VARCHAR(65533) NOT NULL,
        sex VARCHAR(65533) NOT NULL,
        height VARCHAR(65533) NOT NULL,
        weight VARCHAR(65533) NOT NULL,
        date_of_birth VARCHAR(65533) NOT NULL,
        age VARCHAR(65533) NOT NULL,
        key_employee VARCHAR(65533) NOT NULL,
        core_position VARCHAR(65533) NOT NULL,
        cadre_label VARCHAR(65533) NOT NULL,
        expert_label VARCHAR(65533) NOT NULL,
        native_place VARCHAR(65533) NOT NULL,
        nation VARCHAR(65533) NOT NULL,
        marital_status_name VARCHAR(65533) NOT NULL,
        PRIMARY KEY (person_code)
    );
    """
    
    try:
        ddl_id = vn.train(ddl=bi_dataset_ddl)
        print(f"âœ… Successfully added DDL training data: {ddl_id}")
    except Exception as e:
        print(f"âŒ Error adding DDL: {e}")
    
    print("\n2. TRAINING WITH DOCUMENTATION")
    print("-" * 30)
    
    # åŸºäºå®é™…æ•°æ®çš„ä¸šåŠ¡èƒŒæ™¯æ–‡æ¡£
    bi_dataset_documentation = [
        """
        BIæ•°æ®é›†æ•°æ®å­—å…¸ï¼š
        - person_code: å‘˜å·¥å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆå¦‚ï¼š16385, 16388ï¼‰
        - position_name: å²—ä½åç§°ï¼Œä»¥ä¸­æ–‡æ˜¾ç¤ºï¼ˆå¦‚ï¼šå²—ä½56026, å²—ä½74842ï¼‰
        - organization_name: éƒ¨é—¨åç§°ï¼Œä»¥ä¸­æ–‡æ˜¾ç¤ºï¼ˆå¦‚ï¼šéƒ¨é—¨63750, éƒ¨é—¨63616ï¼‰
        - company_name: å…¬å¸/ç®¡æ§å•ä½åç§°ï¼ˆå¦‚ï¼šç®¡æ§å•ä½2017, ç®¡æ§å•ä½2030ï¼‰
        - empl_status: å‘˜å·¥çŠ¶æ€ï¼Œ"åœ¨èŒ"è¡¨ç¤ºåœ¨èŒçŠ¶æ€
        - sex: æ€§åˆ« - "ç”·"ï¼ˆç”·æ€§ï¼‰æˆ–"å¥³"ï¼ˆå¥³æ€§ï¼‰
        - height: èº«é«˜ï¼Œä»¥å˜ç±³ä¸ºå•ä½ï¼Œå¯èƒ½ä¸ºç©º
        - weight: ä½“é‡ï¼Œä»¥å…¬æ–¤ä¸ºå•ä½ï¼Œå¯èƒ½ä¸ºç©º
        - date_of_birth: å‡ºç”Ÿæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD
        - age: å¹´é¾„ï¼Œç”±å‡ºç”Ÿæ—¥æœŸè®¡ç®—å¾—å‡º
        - native_place: ç±è´¯ï¼ˆå¦‚ï¼šé‡åº†å¸‚, å››å·çœï¼‰
        - nation: æ°‘æ—ï¼ˆå¦‚ï¼šæ±‰æ—, åœŸå®¶æ—ï¼‰
        - marital_status_name: å©šå§»çŠ¶å†µ - "å·²å©š"æˆ–"æœªå©š"
        """,
        """
        BIæ•°æ®é›†ä¸šåŠ¡è§„åˆ™ï¼š
        - key_employee: å…³é”®å‘˜å·¥æ ‡è¯†ï¼Œ0=æ™®é€šå‘˜å·¥ï¼Œ1=å…³é”®å‘˜å·¥
        - core_position: æ ¸å¿ƒå²—ä½æ ‡è¯†ï¼Œ0=éæ ¸å¿ƒå²—ä½ï¼Œ1=æ ¸å¿ƒå²—ä½
        - cadre_label: å¹²éƒ¨æ ‡è¯†ï¼Œ0=éç®¡ç†å²—ï¼Œ1=ç®¡ç†å²—
        - expert_label: ä¸“å®¶æ ‡è¯†ï¼Œ0=éä¸“ä¸šæŠ€æœ¯äººå‘˜ï¼Œ1=ä¸“ä¸šæŠ€æœ¯äººå‘˜/ä¸“å®¶
        - èº«é«˜å’Œä½“é‡å­—æ®µå¯èƒ½ä¸ºç©ºå­—ç¬¦ä¸²
        - å¹´é¾„ç”±å‡ºç”Ÿæ—¥æœŸè®¡ç®—å¹¶å­˜å‚¨ä¸ºæ•´æ•°å­—ç¬¦ä¸²
        - æ‰€æœ‰åœ¨èŒå‘˜å·¥çš„empl_statuséƒ½æ˜¯"åœ¨èŒ"
        """,
        """
        æ•°æ®è´¨é‡è¯´æ˜ï¼š
        - éƒ¨åˆ†è®°å½•çš„èº«é«˜/ä½“é‡å­—æ®µä¸ºç©º
        - æ‰€æœ‰åœ¨èŒå‘˜å·¥çš„å‘˜å·¥çŠ¶æ€éƒ½æ˜¯"åœ¨èŒ"
        - å‡ºç”Ÿæ—¥æœŸæ ¼å¼ä¸ºYYYY-MM-DD
        - å²—ä½ã€ç»„ç»‡å’Œå…¬å¸åç§°ä½¿ç”¨ä¸­æ–‡å­—ç¬¦
        - ç±è´¯å’Œæ°‘æ—å­—æ®µåœ¨æŸäº›è®°å½•ä¸­å¯èƒ½ä¸ºç©º
        - æ•°æ®æ¥æºäºä¼ä¸šäººåŠ›èµ„æºç®¡ç†ç³»ç»Ÿ
        """
    ]
    
    for i, doc in enumerate(bi_dataset_documentation):
        try:
            doc_id = vn.train(documentation=doc.strip())
            print(f"âœ… Added documentation {i+1}: {doc_id}")
        except Exception as e:
            print(f"âŒ Error adding documentation {i+1}: {e}")
    
    print("\n3. TRAINING WITH QUESTION-SQL PAIRS")
    print("-" * 30)
    
    # åŸºäºå®é™…bi_datasetç»“æ„çš„ä¸­æ–‡é—®ç­”ç¤ºä¾‹
    training_examples = [
        {
            "question": "ç›®å‰æœ‰å¤šå°‘åœ¨èŒå‘˜å·¥ï¼Ÿ",
            "sql": "SELECT COUNT(*) as active_employees FROM bi_dataset WHERE empl_status = 'åœ¨èŒ'"
        },
        {
            "question": "æŒ‰å‘˜å·¥æ•°é‡æ˜¾ç¤ºå‰5ä¸ªå…¬å¸",
            "sql": """SELECT company_name, COUNT(*) as employee_count 
                     FROM bi_dataset 
                     GROUP BY company_name, company_code 
                     ORDER BY employee_count DESC 
                     LIMIT 5"""
        },
        {
            "question": "æ˜¾ç¤ºæ‰€æœ‰å…³é”®å‘˜å·¥ä¿¡æ¯",
            "sql": """SELECT person_code, position_name, organization_name, company_name, sex, age
                     FROM bi_dataset 
                     WHERE key_employee = '1'
                     ORDER BY age DESC"""
        },
        {
            "question": "ç”·å¥³æ€§åˆ«åˆ†å¸ƒæƒ…å†µå¦‚ä½•ï¼Ÿ",
            "sql": """SELECT sex as 'æ€§åˆ«', COUNT(*) as 'äººæ•°'
                     FROM bi_dataset
                     WHERE sex != '' AND sex IS NOT NULL
                     GROUP BY sex
                     ORDER BY COUNT(*) DESC"""
        },
        {
            "question": "æŸ¥æ‰¾30å²ä»¥ä¸Šåœ¨æ ¸å¿ƒå²—ä½çš„å‘˜å·¥",
            "sql": """SELECT person_code, position_name, age, organization_name, company_name
                     FROM bi_dataset 
                     WHERE CAST(age AS UNSIGNED) > 30 
                     AND core_position = '1'
                     ORDER BY CAST(age AS UNSIGNED) DESC"""
        },
        {
            "question": "å©šå§»çŠ¶å†µçš„åˆ†å¸ƒæƒ…å†µ",
            "sql": """SELECT marital_status_name as 'å©šå§»çŠ¶å†µ', COUNT(*) as 'äººæ•°'
                     FROM bi_dataset
                     WHERE marital_status_name != '' AND marital_status_name IS NOT NULL
                     GROUP BY marital_status_name
                     ORDER BY COUNT(*) DESC"""
        },
        {
            "question": "æŸ¥æ‰¾æ¥è‡ªé‡åº†å¸‚çš„å‘˜å·¥",
            "sql": """SELECT person_code, position_name, native_place, organization_name, company_name, age
                     FROM bi_dataset
                     WHERE native_place = 'é‡åº†å¸‚'
                     ORDER BY CAST(age AS UNSIGNED) DESC"""
        },
        {
            "question": "ç»Ÿè®¡å„ä¸ªéƒ¨é—¨çš„å‘˜å·¥æ•°é‡",
            "sql": """SELECT organization_name as 'éƒ¨é—¨åç§°', COUNT(*) as 'å‘˜å·¥æ•°é‡'
                     FROM bi_dataset
                     GROUP BY organization_name, organization_code
                     ORDER BY COUNT(*) DESC"""
        }
    ]
    
    for i, example in enumerate(training_examples):
        try:
            q_id = vn.train(question=example["question"], sql=example["sql"])
            print(f"âœ… Added Q&A {i+1}: {q_id}")
            print(f"   Question: {example['question'][:60]}...")
        except Exception as e:
            print(f"âŒ Error adding Q&A {i+1}: {e}")
    
    print("\n4. TRAINING DATA MANAGEMENT")
    print("-" * 30)
    
    # Show current training data
    training_data = display_training_data_summary(vn)
    
    # Test training data removal (if there's data to remove)
    if len(training_data) > 0:
        print(f"\nTesting training data removal...")
        first_id = training_data.iloc[0]['id'] if 'id' in training_data.columns else None
        if first_id:
            try:
                removed = vn.remove_training_data(first_id)
                print(f"âœ… Removal result: {removed}")
            except Exception as e:
                print(f"âŒ Error removing training data: {e}")
    else:
        print("No training data available for removal testing")
    
    print("\n5. ADVANCED TRAINING SCENARIOS")
    print("-" * 30)
    
    # é’ˆå¯¹bi_datasetçš„å¤æ‚ä¸­æ–‡SQLæŸ¥è¯¢ç¤ºä¾‹
    advanced_examples = [
        {
            "question": "è®¡ç®—å„éƒ¨é—¨å¹³å‡å¹´é¾„ï¼Œåªæ˜¾ç¤ºäººæ•°è¶…è¿‡3äººçš„éƒ¨é—¨",
            "sql": """SELECT organization_name as 'éƒ¨é—¨åç§°', 
                            COUNT(*) as 'å‘˜å·¥æ•°é‡',
                            ROUND(AVG(CAST(age AS UNSIGNED)), 1) as 'å¹³å‡å¹´é¾„'
                     FROM bi_dataset 
                     WHERE age != '' AND age REGEXP '^[0-9]+$' AND empl_status = 'åœ¨èŒ'
                     GROUP BY organization_name, organization_code
                     HAVING COUNT(*) > 3
                     ORDER BY AVG(CAST(age AS UNSIGNED)) DESC"""
        },
        {
            "question": "æŸ¥æ‰¾æ¯ä¸ªå…¬å¸æœ€å¹´è½»å’Œæœ€å¹´é•¿çš„å‘˜å·¥å¹´é¾„åŠå¹´é¾„è·¨åº¦",
            "sql": """SELECT company_name as 'å…¬å¸åç§°',
                            COUNT(*) as 'å‘˜å·¥æ€»æ•°',
                            MIN(CAST(age AS UNSIGNED)) as 'æœ€å°å¹´é¾„',
                            MAX(CAST(age AS UNSIGNED)) as 'æœ€å¤§å¹´é¾„',
                            MAX(CAST(age AS UNSIGNED)) - MIN(CAST(age AS UNSIGNED)) as 'å¹´é¾„è·¨åº¦'
                     FROM bi_dataset
                     WHERE age != '' AND age REGEXP '^[0-9]+$' AND empl_status = 'åœ¨èŒ'
                     GROUP BY company_name, company_code
                     HAVING COUNT(*) > 2
                     ORDER BY MAX(CAST(age AS UNSIGNED)) - MIN(CAST(age AS UNSIGNED)) DESC"""
        },
        {
            "question": "ç»Ÿè®¡å„æ°‘æ—å‘˜å·¥æ•°é‡åˆ†å¸ƒæƒ…å†µ",
            "sql": """SELECT nation as 'æ°‘æ—', COUNT(*) as 'äººæ•°',
                            ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM bi_dataset WHERE nation != '' AND nation IS NOT NULL), 2) as 'ç™¾åˆ†æ¯”'
                     FROM bi_dataset
                     WHERE nation != '' AND nation IS NOT NULL AND empl_status = 'åœ¨èŒ'
                     GROUP BY nation
                     ORDER BY COUNT(*) DESC"""
        },
        {
            "question": "æŸ¥æ‰¾å…·æœ‰å®Œæ•´èº«ä½“æ•°æ®ï¼ˆèº«é«˜å’Œä½“é‡ï¼‰çš„å‘˜å·¥ä¿¡æ¯",
            "sql": """SELECT person_code as 'å‘˜å·¥ç¼–å·', position_name as 'å²—ä½åç§°', 
                            organization_name as 'éƒ¨é—¨åç§°', sex as 'æ€§åˆ«', 
                            height as 'èº«é«˜(cm)', weight as 'ä½“é‡(kg)', age as 'å¹´é¾„'
                     FROM bi_dataset
                     WHERE height != '' AND height IS NOT NULL AND height REGEXP '^[0-9]+$'
                     AND weight != '' AND weight IS NOT NULL AND weight REGEXP '^[0-9]+$'
                     AND empl_status = 'åœ¨èŒ'
                     ORDER BY CAST(age AS UNSIGNED) DESC"""
        },
        {
            "question": "åˆ†æå¹²éƒ¨å’Œä¸“å®¶åœ¨å„å…¬å¸çš„åˆ†å¸ƒæƒ…å†µ",
            "sql": """SELECT company_name as 'å…¬å¸åç§°',
                            COUNT(*) as 'å‘˜å·¥æ€»æ•°',
                            SUM(CASE WHEN cadre_label = '1' THEN 1 ELSE 0 END) as 'å¹²éƒ¨æ•°é‡',
                            SUM(CASE WHEN expert_label = '1' THEN 1 ELSE 0 END) as 'ä¸“å®¶æ•°é‡',
                            ROUND(SUM(CASE WHEN cadre_label = '1' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) as 'å¹²éƒ¨æ¯”ä¾‹%',
                            ROUND(SUM(CASE WHEN expert_label = '1' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) as 'ä¸“å®¶æ¯”ä¾‹%'
                     FROM bi_dataset
                     WHERE empl_status = 'åœ¨èŒ'
                     GROUP BY company_name, company_code
                     HAVING COUNT(*) > 5
                     ORDER BY COUNT(*) DESC"""
        }
    ]
    
    for i, example in enumerate(advanced_examples):
        try:
            adv_id = vn.train(question=example["question"], sql=example["sql"])
            print(f"âœ… Added advanced Q&A {i+1}: {adv_id}")
        except Exception as e:
            print(f"âŒ Error adding advanced Q&A {i+1}: {e}")
    
    print("\n6. TESTING SQL GENERATION")
    print("-" * 30)
    
    # ä½¿ç”¨è®­ç»ƒæ¨¡å‹æµ‹è¯•SQLç”Ÿæˆ
    test_questions = [
        "æœ‰å¤šå°‘åœ¨èŒå‘˜å·¥ï¼Ÿ",
        "æŒ‰éƒ¨é—¨æ˜¾ç¤ºå…³é”®å‘˜å·¥", 
        "ç”·å¥³æ€§åˆ«åˆ†å¸ƒæƒ…å†µ",
        "æŸ¥æ‰¾æ¥è‡ªå››å·çœçš„å‘˜å·¥",
        "æ˜¾ç¤ºå·²å©šå‘˜å·¥ä¿¡æ¯",
        "æ ¸å¿ƒå²—ä½æœ‰å¤šå°‘äººï¼Ÿ",
        "å¹²éƒ¨çš„å¹³å‡å¹´é¾„æ˜¯å¤šå°‘ï¼Ÿ"
    ]
    
    for question in test_questions:
        print(f"\nğŸ¤– Testing question: '{question}'")
        try:
            sql = vn.generate_sql(question)
            print(f"Generated SQL: {sql}")
        except Exception as e:
            print(f"âŒ Error generating SQL: {e}")
    
    print("\n7. FINAL TRAINING DATA SUMMARY")
    print("-" * 30)
    final_training_data = display_training_data_summary(vn)
    
    print(f"\nğŸ“Š TRAINING INSIGHTS:")
    print(f"   - Schema training provides structure understanding")
    print(f"   - Documentation adds business context and rules")
    print(f"   - Question-SQL pairs enable pattern learning")
    print(f"   - Training data is stored in Qdrant vector database")
    print(f"   - Generated SQL uses trained context for accuracy")
    
    return vn


def demonstrate_training_best_practices():
    """Demonstrate best practices for training Vanna"""
    
    print("\n\nğŸ¯ VANNA TRAINING BEST PRACTICES")
    print("=" * 50)
    
    practices = [
        {
            "practice": "Start with DDL",
            "description": "Always begin training with CREATE TABLE statements to establish schema knowledge",
            "example": "Train with complete table definitions including constraints and relationships"
        },
        {
            "practice": "Add Business Context", 
            "description": "Include documentation about business rules, data meanings, and constraints",
            "example": "Document what 'key_employee' means in business terms"
        },
        {
            "practice": "Provide Diverse Examples",
            "description": "Train with various SQL patterns: aggregations, joins, filtering, sorting",
            "example": "Include simple SELECTs, GROUP BY queries, and complex analytical queries"
        },
        {
            "practice": "Use Representative Data",
            "description": "Ensure training examples reflect real-world query patterns",
            "example": "Include common business questions and their SQL solutions"
        },
        {
            "practice": "Iterative Training",
            "description": "Train incrementally and test, then add more examples based on gaps",
            "example": "Start with basic queries, then add complex analytical examples"
        },
        {
            "practice": "Training Data Hygiene",
            "description": "Regularly review and clean training data, remove obsolete examples",
            "example": "Remove outdated schema definitions when tables are restructured"
        }
    ]
    
    for i, practice in enumerate(practices, 1):
        print(f"\n{i}. {practice['practice'].upper()}")
        print(f"   Description: {practice['description']}")
        print(f"   Example: {practice['example']}")


def test_ask_functionality(vn):
    """Test the full ask() functionality including SQL execution"""
    print("\n8. TESTING FULL ASK() FUNCTIONALITY")
    print("-" * 30)
    
    # é€‚ç”¨äºbi_datasetçš„ä¸­æ–‡æµ‹è¯•é—®é¢˜
    ask_questions = [
        "æ•°æ®åº“ä¸­æœ‰å¤šå°‘å‘˜å·¥ï¼Ÿ",
        "æ˜¾ç¤ºæ€§åˆ«åˆ†å¸ƒæƒ…å†µ",
        "æˆ‘ä»¬æ•°æ®åº“ä¸­æœ‰å“ªäº›å…¬å¸ï¼Ÿ",
        "æŸ¥æ‰¾æ‰€æœ‰å…³é”®å‘˜å·¥",
        "æ˜¾ç¤ºæ¥è‡ªé‡åº†å¸‚çš„å‘˜å·¥",
        "å„ä¸ªæ°‘æ—å„æœ‰å¤šå°‘äººï¼Ÿ",
        "å¹´é¾„æœ€å¤§çš„å‘˜å·¥æ˜¯è°ï¼Ÿ"
    ]
    
    for question in ask_questions:
        print(f"\nğŸ’¬ Asking: '{question}'")
        try:
            # Use ask() which generates SQL and can execute it
            result = vn.ask(question, print_results=True, auto_train=True)
            if result:
                sql, df, plot = result
                print(f"   SQL: {sql}")
                if df is not None and len(df) > 0:
                    print(f"   Results: {len(df)} rows returned")
                else:
                    print("   No results returned")
        except Exception as e:
            print(f"   âŒ Error with ask(): {e}")


if __name__ == "__main__":
    print("ğŸš€ Starting Vanna Training Investigation with Real Connections...")
    
    # Set up Vanna connection
    vn = setup_vanna_connection()
    
    if vn is None:
        print("âŒ Could not establish Vanna connection. Please check:")
        print("   - Qdrant is running on localhost:6333")
        print("   - OpenAI API credentials are valid")
        print("   - MySQL database is accessible")
        exit(1)
    
    try:
        # Demonstrate training with employee data
        print("\n" + "="*60)
        trained_vanna = demonstrate_employee_data_training(vn)
        
        # Test full ask functionality
        test_ask_functionality(trained_vanna)
        
        # Show best practices
        demonstrate_training_best_practices()
        
        print("\n\nâœ¨ INVESTIGATION COMPLETE!")
        print("This script demonstrates real Vanna training capabilities with:")
        print("âœ… OpenAI LLM integration")
        print("âœ… Qdrant vector database storage") 
        print("âœ… MySQL database connection")
        print("âœ… DDL, documentation, and Q&A training")
        print("âœ… SQL generation and execution")
        
    except Exception as e:
        print(f"\nâŒ Investigation failed: {e}")
        print("Check your connections and credentials")