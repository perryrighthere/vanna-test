# VannaæŠ€æœ¯è°ƒç ”æŠ¥å‘Š

## ç›®å½•
- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [æ•´ä½“æ¶æ„è®¾è®¡](#1-æ•´ä½“æ¶æ„è®¾è®¡)
- [æ ¸å¿ƒåŸºç±»å®ç°](#2-æ ¸å¿ƒåŸºç±»vannabaseå®ç°)
- [LLMé›†æˆæ¨¡å—](#3-llmé›†æˆæ¨¡å—å®ç°)
- [å‘é‡æ•°æ®åº“RAGå®ç°](#4-å‘é‡æ•°æ®åº“ragå®ç°)
- [è®­ç»ƒæœºåˆ¶å’Œæ•°æ®æµ](#5-trainingè®­ç»ƒæœºåˆ¶å’Œæ•°æ®æµ)
- [æŸ¥è¯¢æµç¨‹å’Œæ£€ç´¢è¿‡ç¨‹](#6-askæŸ¥è¯¢æµç¨‹å’Œragæ£€ç´¢è¿‡ç¨‹)
- [æŠ€æœ¯åˆ›æ–°ç‚¹](#7-æŠ€æœ¯åˆ›æ–°ç‚¹)
- [æŠ€æœ¯å®ç°æ¡†æ¶æ€»ç»“](#8-æŠ€æœ¯å®ç°æ¡†æ¶æ€»ç»“)

---

## é¡¹ç›®æ¦‚è¿°

**Vanna** æ˜¯ä¸€ä¸ªåŸºäºRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯çš„å¼€æºPythonæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè‡ªç„¶è¯­è¨€åˆ°SQLçš„è½¬æ¢ã€‚è¯¥é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ï¼Œé€šè¿‡ç»“åˆå‘é‡æ•°æ®åº“å’Œå¤§è¯­è¨€æ¨¡å‹æ¥æ˜¾è‘—æé«˜text-to-SQLçš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚

### æ ¸å¿ƒç‰¹æ€§
- ğŸ¤– æ”¯æŒ30+ç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆOpenAIã€Anthropicã€Googleç­‰ï¼‰
- ğŸ—„ï¸ é›†æˆå¤šç§å‘é‡æ•°æ®åº“ï¼ˆChromaDBã€Pineconeã€Weaviateç­‰ï¼‰
- ğŸ§  åŸºäºRAGçš„æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢
- ğŸ“Š è‡ªåŠ¨å¯è§†åŒ–ç”Ÿæˆ
- ğŸ”„ å¢é‡å­¦ä¹ å’Œè‡ªæˆ‘ä¼˜åŒ–

---

## 1. æ•´ä½“æ¶æ„è®¾è®¡

### 1.1 æ ¸å¿ƒæ¶æ„æ¨¡å¼

Vannaé‡‡ç”¨**å¤šé‡ç»§æ‰¿çš„æ¨¡å—åŒ–æ¶æ„**ï¼Œå®ç°äº†LLMå’Œå‘é‡æ•°æ®åº“çš„å®Œå…¨è§£è€¦ï¼š

```mermaid
graph TB
    subgraph "Vannaæ ¸å¿ƒæ¶æ„"
        A[VannaBaseæŠ½è±¡åŸºç±»] --> B[LLMé›†æˆç»„ä»¶]
        A --> C[å‘é‡æ•°æ®åº“ç»„ä»¶]
        
        subgraph "LLMå®ç°"
            B --> D[OpenAI_Chat]
            B --> E[Anthropic_Chat]
            B --> F[Google_Chat]
            B --> G[å…¶ä»–LLM...]
        end
        
        subgraph "å‘é‡æ•°æ®åº“å®ç°"
            C --> H[ChromaDB_VectorStore]
            C --> I[PGVector_VectorStore]
            C --> J[Pinecone_VectorStore]
            C --> K[å…¶ä»–å‘é‡DB...]
        end
    end
    
    subgraph "ç”¨æˆ·å®ç°"
        L[MyVannaç±»] --> D
        L --> H
    end
```

### 1.2 å®ç°ç¤ºä¾‹

```python
from vanna.openai.openai_chat import OpenAI_Chat
from vanna.chromadb.chromadb_vector import ChromaDB_VectorStore

class MyVanna(ChromaDB_VectorStore, OpenAI_Chat):
    def __init__(self, config=None):
        ChromaDB_VectorStore.__init__(self, config=config)
        OpenAI_Chat.__init__(self, config=config)
```

### 1.3 é¡¹ç›®ç›®å½•ç»“æ„

```
vanna/
â”œâ”€â”€ src/vanna/
â”‚   â”œâ”€â”€ base/                    # æ ¸å¿ƒæŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ openai/                 # OpenAIé›†æˆ (GPT-3.5/4)
â”‚   â”œâ”€â”€ anthropic/              # Anthropicé›†æˆ (Claude)
â”‚   â”œâ”€â”€ google/                 # Googleé›†æˆ (Gemini)
â”‚   â”œâ”€â”€ chromadb/               # ChromaDBå‘é‡å­˜å‚¨
â”‚   â”œâ”€â”€ pgvector/               # PostgreSQLå‘é‡æ‰©å±•
â”‚   â”œâ”€â”€ pinecone/               # Pineconeå‘é‡æ•°æ®åº“
â”‚   â”œâ”€â”€ weaviate/               # Weaviateå‘é‡æ•°æ®åº“
â”‚   â””â”€â”€ [30+ å…¶ä»–é›†æˆç›®å½•]
â”œâ”€â”€ training_data/              # ç¤ºä¾‹è®­ç»ƒæ•°æ®é›†
â”‚   â”œâ”€â”€ sample-salaries/        # è–ªèµ„æ•°æ®ç¤ºä¾‹
â”‚   â”œâ”€â”€ sample-fraud/           # æ¬ºè¯ˆæ£€æµ‹ç¤ºä¾‹
â”‚   â””â”€â”€ tpc-h/                  # TPC-HåŸºå‡†æµ‹è¯•
â””â”€â”€ tests/                      # å•å…ƒæµ‹è¯•
```

---

## 2. æ ¸å¿ƒåŸºç±»VannaBaseå®ç°

### 2.1 è®¾è®¡ç†å¿µ

`VannaBase` (`src/vanna/base/base.py:72`) æ˜¯æ•´ä¸ªæ¡†æ¶çš„æ ¸å¿ƒæŠ½è±¡ç±»ï¼Œå®šä¹‰äº†æ ‡å‡†åŒ–çš„æ–¹æ³•å‘½åè§„èŒƒå’Œæ¥å£ï¼š

```mermaid
graph LR
    subgraph "VannaBaseæ–¹æ³•ä½“ç³»"
        A[vn.get_*<br/>è·å–æ•°æ®] --> B[vn.add_*<br/>æ·»åŠ è®­ç»ƒæ•°æ®]
        B --> C[vn.generate_*<br/>AIç”Ÿæˆå†…å®¹]
        C --> D[vn.run_*<br/>æ‰§è¡ŒSQL]
        D --> E[vn.connect_*<br/>æ•°æ®åº“è¿æ¥]
        E --> F[vn.remove_*<br/>ç§»é™¤æ•°æ®]
    end
```

### 2.2 å…³é”®æŠ½è±¡æ–¹æ³•

```python
class VannaBase(ABC):
    # å‘é‡åµŒå…¥ç”Ÿæˆ
    @abstractmethod
    def generate_embedding(self, data: str) -> List[float]:
        pass
    
    # RAGæ£€ç´¢æ ¸å¿ƒæ–¹æ³•
    @abstractmethod
    def get_similar_question_sql(self, question: str) -> list:
        pass
    
    @abstractmethod
    def get_related_ddl(self, question: str) -> list:
        pass
    
    @abstractmethod
    def get_related_documentation(self, question: str) -> list:
        pass
    
    # LLMäº¤äº’
    @abstractmethod
    def submit_prompt(self, prompt) -> str:
        pass
```

### 2.3 æ ¸å¿ƒä¸šåŠ¡æµç¨‹

```mermaid
sequenceDiagram
    participant U as User
    participant V as VannaBase
    participant L as LLM
    participant D as VectorDB
    participant S as SQL DB
    
    U->>V: ask("è‡ªç„¶è¯­è¨€é—®é¢˜")
    V->>D: get_similar_question_sql()
    V->>D: get_related_ddl()
    V->>D: get_related_documentation()
    D-->>V: è¿”å›ç›¸å…³ä¸Šä¸‹æ–‡
    V->>V: construct_prompt()
    V->>L: submit_prompt()
    L-->>V: ç”ŸæˆSQL
    V->>S: run_sql()
    S-->>V: æŸ¥è¯¢ç»“æœ
    V->>V: generate_visualization()
    V-->>U: è¿”å›SQLã€ç»“æœã€å›¾è¡¨
```

---

## 3. LLMé›†æˆæ¨¡å—å®ç°

### 3.1 OpenAIé›†æˆ (`src/vanna/openai/openai_chat.py`)

```python
class OpenAI_Chat(VannaBase):
    def __init__(self, client=None, config=None):
        self.temperature = 0.7  # å¯é…ç½®åˆ›é€ æ€§
        self.client = OpenAI(api_key=config.get("api_key"))
    
    def submit_prompt(self, prompt, **kwargs) -> str:
        # æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼šæ ¹æ®tokenæ•°é‡é€‰æ‹©åˆé€‚æ¨¡å‹
        if num_tokens > 3500:
            model = "gpt-3.5-turbo-16k"
        else:
            model = "gpt-3.5-turbo"
            
        response = self.client.chat.completions.create(
            model=model,
            messages=prompt,
            temperature=self.temperature
        )
        return response.choices[0].message.content
```

### 3.2 Anthropicé›†æˆ (`src/vanna/anthropic/anthropic_chat.py`)

```python
class Anthropic_Chat(VannaBase):
    def submit_prompt(self, prompt, **kwargs) -> str:
        # Claudeç‰¹æ®Šå¤„ç†ï¼šåˆ†ç¦»system message
        system_message = ''
        no_system_prompt = []
        for prompt_message in prompt:
            if prompt_message['role'] == 'system':
                system_message = prompt_message['content']
            else:
                no_system_prompt.append(prompt_message)
                
        response = self.client.messages.create(
            model=self.config["model"],
            messages=no_system_prompt,
            system=system_message,
            max_tokens=self.max_tokens
        )
        return response.content[0].text
```

### 3.3 LLMæ¶æ„å›¾

```mermaid
graph TD
    A[VannaBase] --> B[LLMæŠ½è±¡æ¥å£]
    B --> C[OpenAI_Chat<br/>GPT-3.5/4ç³»åˆ—]
    B --> D[Anthropic_Chat<br/>Claudeç³»åˆ—]
    B --> E[Google_Chat<br/>Geminiç³»åˆ—]
    B --> F[Ollama_Chat<br/>æœ¬åœ°æ¨¡å‹]
    
    C --> C1[æ™ºèƒ½æ¨¡å‹é€‰æ‹©<br/>tokenè®¡æ•°]
    D --> D1[Systemæ¶ˆæ¯ç‰¹æ®Šå¤„ç†]
    E --> E1[å¤šæ¨¡æ€æ”¯æŒ]
    F --> F1[æœ¬åœ°éƒ¨ç½²æ”¯æŒ]
```

---

## 4. å‘é‡æ•°æ®åº“RAGå®ç°

### 4.1 RAGä¸‰ç»´æ£€ç´¢æ¶æ„

Vannaå®ç°äº†ç‹¬ç‰¹çš„**ä¸‰ç»´RAGæ£€ç´¢ç³»ç»Ÿ**ï¼š

```mermaid
graph TB
    subgraph "RAGä¸‰ç»´æ£€ç´¢ç³»ç»Ÿ"
        A[ç”¨æˆ·é—®é¢˜] --> B[å‘é‡åŒ–]
        B --> C[ç›¸ä¼¼åº¦æ£€ç´¢]
        
        C --> D[Question-SQLé›†åˆ<br/>å†å²é—®ç­”å¯¹]
        C --> E[DDLé›†åˆ<br/>è¡¨ç»“æ„ä¿¡æ¯] 
        C --> F[Documentationé›†åˆ<br/>ä¸šåŠ¡ä¸Šä¸‹æ–‡]
        
        D --> G[ç»„è£…ä¸Šä¸‹æ–‡]
        E --> G
        F --> G
        
        G --> H[æ„å»ºPrompt]
        H --> I[LLMç”ŸæˆSQL]
    end
```

### 4.2 ChromaDBå®ç°è¯¦è§£ (`src/vanna/chromadb/chromadb_vector.py`)

```python
class ChromaDB_VectorStore(VannaBase):
    def __init__(self, config=None):
        # ä¸‰ä¸ªç‹¬ç«‹é›†åˆ
        self.documentation_collection = self.chroma_client.get_or_create_collection(
            name="documentation", embedding_function=self.embedding_function)
        self.ddl_collection = self.chroma_client.get_or_create_collection(
            name="ddl", embedding_function=self.embedding_function)
        self.sql_collection = self.chroma_client.get_or_create_collection(
            name="sql", embedding_function=self.embedding_function)
    
    def add_question_sql(self, question: str, sql: str) -> str:
        # ç¡®å®šæ€§IDç”Ÿæˆï¼Œé¿å…é‡å¤
        question_sql_json = json.dumps({
            "question": question, "sql": sql
        }, ensure_ascii=False)
        id = deterministic_uuid(question_sql_json) + "-sql"
        
        self.sql_collection.add(
            documents=question_sql_json,
            embeddings=self.generate_embedding(question_sql_json),
            ids=id
        )
        return id
```

### 4.3 å‘é‡æ•°æ®åº“ç”Ÿæ€æ”¯æŒ

```mermaid
graph LR
    subgraph "å‘é‡æ•°æ®åº“æ”¯æŒ"
        A[VectorStoreæŠ½è±¡å±‚] --> B[ChromaDB<br/>å¼€æºè½»é‡çº§]
        A --> C[PGVector<br/>PostgreSQLæ‰©å±•]
        A --> D[Pinecone<br/>äº‘åŸç”Ÿå‘é‡DB]
        A --> E[Weaviate<br/>å›¾è°±å‘é‡DB]
        A --> F[Milvus<br/>åˆ†å¸ƒå¼å‘é‡DB]
        A --> G[Qdrant<br/>é«˜æ€§èƒ½å‘é‡DB]
    end
    
    subgraph "ç‰¹æ€§å¯¹æ¯”"
        B --> B1[æœ¬åœ°éƒ¨ç½²<br/>å¿«é€ŸåŸå‹]
        C --> C1[SQLç”Ÿæ€<br/>äº‹åŠ¡æ”¯æŒ]
        D --> D1[æ‰˜ç®¡æœåŠ¡<br/>è‡ªåŠ¨ç¼©æ”¾]
        E --> E1[è¯­ä¹‰æœç´¢<br/>å›¾è°±é›†æˆ]
    end
```

### 4.4 PGVectorå®ç° (`src/vanna/pgvector/pgvector.py`)

```python
class PG_VectorStore(VannaBase):
    def __init__(self, config=None):
        # ä½¿ç”¨LangChainçš„PGVectorå°è£…
        self.sql_collection = PGVector(
            embeddings=self.embedding_function,
            collection_name="sql",
            connection=self.connection_string
        )
        
        # é»˜è®¤ä½¿ç”¨HuggingFace embeddings
        if "embedding_function" not in config:
            from langchain_huggingface import HuggingFaceEmbeddings
            self.embedding_function = HuggingFaceEmbeddings(
                model_name="all-MiniLM-L6-v2"
            )
```

---

## 5. Trainingè®­ç»ƒæœºåˆ¶å’Œæ•°æ®æµ

### 5.1 è®­ç»ƒæ•°æ®ç±»å‹å’Œæµç¨‹

```mermaid
graph TD
    subgraph "è®­ç»ƒæ•°æ®ç±»å‹"
        A[Question-SQLå¯¹<br/>ç”¨æˆ·é—®é¢˜+SQL] --> D[å‘é‡åŒ–å­˜å‚¨]
        B[DDLè¯­å¥<br/>è¡¨ç»“æ„å®šä¹‰] --> D
        C[Documentation<br/>ä¸šåŠ¡é€»è¾‘è¯´æ˜] --> D
    end
    
    subgraph "è‡ªåŠ¨è®­ç»ƒæµç¨‹"
        E[ç”¨æˆ·æŸ¥è¯¢æˆåŠŸ] --> F[è‡ªåŠ¨æ·»åŠ åˆ°è®­ç»ƒé›†]
        F --> G[å‘é‡åŒ–å¹¶å­˜å‚¨]
        G --> H[ä¸‹æ¬¡æŸ¥è¯¢å¯ç”¨]
    end
    
    subgraph "è®­ç»ƒè®¡åˆ’ç”Ÿæˆ"
        I[æ•°æ®åº“è¿æ¥] --> J[è·å–INFORMATION_SCHEMA]
        J --> K[ç”ŸæˆDDLè®­ç»ƒæ•°æ®]
        J --> L[æå–å†å²æŸ¥è¯¢]
        L --> M[ç”ŸæˆQuestion-SQLå¯¹]
    end
    
    D --> N[RAGæ£€ç´¢æ± ]
    H --> N
    K --> N
    M --> N
```

### 5.2 è®­ç»ƒæ–¹æ³•å®ç° (`src/vanna/base/base.py:1796`)

```python
def train(self, question=None, sql=None, ddl=None, documentation=None, plan=None):
    """ç»Ÿä¸€çš„è®­ç»ƒæ¥å£"""
    
    if documentation:
        print("Adding documentation....")
        return self.add_documentation(documentation)
    
    if sql:
        if question is None:
            # è‡ªåŠ¨ä»SQLç”Ÿæˆé—®é¢˜
            question = self.generate_question(sql)
            print(f"Question generated with sql: {question}")
        return self.add_question_sql(question=question, sql=sql)
    
    if ddl:
        print(f"Adding ddl: {ddl}")
        return self.add_ddl(ddl)
    
    if plan:
        # æ‰¹é‡è®­ç»ƒè®¡åˆ’æ‰§è¡Œ
        for item in plan._plan:
            if item.item_type == TrainingPlanItem.ITEM_TYPE_DDL:
                self.add_ddl(item.item_value)
            elif item.item_type == TrainingPlanItem.ITEM_TYPE_IS:
                self.add_documentation(item.item_value)
            elif item.item_type == TrainingPlanItem.ITEM_TYPE_SQL:
                self.add_question_sql(question=item.item_name, sql=item.item_value)
```

### 5.3 è‡ªåŠ¨è®­ç»ƒè®¡åˆ’ç”Ÿæˆ

```python
def get_training_plan_snowflake(self, filter_databases=None, use_historical_queries=True):
    """Snowflakeä¸“ç”¨è®­ç»ƒè®¡åˆ’"""
    plan = TrainingPlan([])
    
    if use_historical_queries:
        # æå–å†å²æŸ¥è¯¢ä½œä¸ºè®­ç»ƒæ•°æ®
        df_history = self.run_sql("""
            SELECT * FROM table(information_schema.query_history(result_limit => 5000)) 
            ORDER BY start_time
        """)
        
        for query in df_history["QUERY_TEXT"].unique().tolist():
            plan._plan.append(TrainingPlanItem(
                item_type=TrainingPlanItem.ITEM_TYPE_SQL,
                item_name=self.generate_question(query),
                item_value=query
            ))
    
    return plan
```

### 5.4 è®­ç»ƒæ•°æ®ç¤ºä¾‹ç»“æ„

```json
// training_data/sample-salaries/questions.json
[
    {
        "question": "What is the engineer to product manager ratio in Facebook, Amazon, Google?",
        "answer": "SELECT company, count(case when title like '%Engineer%' then 1 else null end) as engineer_count, count(case when title like '%Product Manager%' then 1 else null end) as product_manager_count FROM salaries_data WHERE company in ('Facebook', 'Amazon', 'Google') GROUP BY company"
    }
]
```

---

## 6. AskæŸ¥è¯¢æµç¨‹å’ŒRAGæ£€ç´¢è¿‡ç¨‹

### 6.1 å®Œæ•´æŸ¥è¯¢æµç¨‹æ¶æ„

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant V as Vanna
    participant R as RAGæ£€ç´¢å™¨
    participant L as LLM
    participant D as æ•°æ®åº“
    participant VIZ as å¯è§†åŒ–å¼•æ“
    
    U->>V: ask("è‡ªç„¶è¯­è¨€é—®é¢˜")
    
    Note over V,R: RAGæ£€ç´¢é˜¶æ®µ
    V->>R: get_similar_question_sql(question)
    R-->>V: ç›¸ä¼¼é—®ç­”å¯¹åˆ—è¡¨
    V->>R: get_related_ddl(question)  
    R-->>V: ç›¸å…³è¡¨ç»“æ„
    V->>R: get_related_documentation(question)
    R-->>V: ä¸šåŠ¡ä¸Šä¸‹æ–‡æ–‡æ¡£
    
    Note over V,L: Promptæ„å»ºä¸LLMç”Ÿæˆ
    V->>V: construct_sql_prompt(context)
    V->>L: submit_prompt(prompt)
    L-->>V: ç”Ÿæˆçš„SQLæŸ¥è¯¢
    
    Note over V: SQLéªŒè¯ä¸æ‰§è¡Œ
    V->>V: extract_sql(response)
    V->>V: is_sql_valid(sql)
    
    alt SQLæœ‰æ•ˆ
        V->>D: run_sql(sql)
        D-->>V: æŸ¥è¯¢ç»“æœDataFrame
        
        Note over V: è‡ªåŠ¨è®­ç»ƒ
        V->>V: add_question_sql(question, sql)
        
        Note over V,VIZ: å¯è§†åŒ–ç”Ÿæˆ
        V->>VIZ: generate_plotly_code(question, sql, df)
        VIZ-->>V: Plotlyå›¾è¡¨ä»£ç 
        V->>V: get_plotly_figure(code, df)
        V-->>U: SQL + ç»“æœ + å›¾è¡¨
    else SQLæ— æ•ˆ
        V-->>U: é”™è¯¯ä¿¡æ¯
    end
```

### 6.2 æ ¸å¿ƒgenerate_sqlæ–¹æ³•è§£æ (`src/vanna/base/base.py:93`)

```python
def generate_sql(self, question: str, allow_llm_to_see_data=False) -> str:
    """RAGå¢å¼ºçš„SQLç”Ÿæˆ"""
    
    # 1. RAGä¸‰ç»´æ£€ç´¢
    question_sql_list = self.get_similar_question_sql(question)
    ddl_list = self.get_related_ddl(question) 
    doc_list = self.get_related_documentation(question)
    
    # 2. åŠ¨æ€Promptæ„å»º
    prompt = self.get_sql_prompt(
        initial_prompt=self.config.get("initial_prompt"),
        question=question,
        question_sql_list=question_sql_list,
        ddl_list=ddl_list,
        doc_list=doc_list
    )
    
    # 3. LLMç”ŸæˆSQL
    llm_response = self.submit_prompt(prompt)
    
    # 4. æ™ºèƒ½ä¸­é—´æŸ¥è¯¢å¤„ç†
    if 'intermediate_sql' in llm_response and allow_llm_to_see_data:
        # æ‰§è¡Œä¸­é—´SQLæ¢ç´¢æ•°æ®
        intermediate_sql = self.extract_sql(llm_response)
        df = self.run_sql(intermediate_sql)
        
        # é‡æ–°æ„å»ºåŒ…å«æ•°æ®ä¸Šä¸‹æ–‡çš„Prompt
        enhanced_doc_list = doc_list + [
            f"Intermediate SQL query results: \n{df.to_markdown()}"
        ]
        prompt = self.get_sql_prompt(question=question, doc_list=enhanced_doc_list)
        llm_response = self.submit_prompt(prompt)
    
    return self.extract_sql(llm_response)
```

### 6.3 æ™ºèƒ½Promptå·¥ç¨‹

```mermaid
graph TB
    subgraph "Promptæ„å»ºç­–ç•¥"
        A[åˆå§‹ç³»ç»Ÿæç¤º] --> B[æ·»åŠ DDLä¿¡æ¯<br/>add_ddl_to_prompt]
        B --> C[æ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡<br/>add_documentation_to_prompt] 
        C --> D[æ·»åŠ ç¤ºä¾‹SQL<br/>add_sql_to_prompt]
        D --> E[Tokené™åˆ¶ç®¡ç†<br/>max_tokens=14000]
    end
    
    subgraph "ä¸Šä¸‹æ–‡ä¼˜å…ˆçº§"
        F[æœ€ç›¸å…³çš„Question-SQLå¯¹] --> G[é«˜ä¼˜å…ˆçº§]
        H[å…³é”®è¡¨ç»“æ„DDL] --> G
        I[æ ¸å¿ƒä¸šåŠ¡æ–‡æ¡£] --> G
        J[é™æ€æ–‡æ¡£] --> K[ä½ä¼˜å…ˆçº§]
    end
    
    E --> L[æœ€ç»ˆPrompt]
    G --> L
    K --> L
```

### 6.4 è‡ªåŠ¨è®­ç»ƒåé¦ˆå¾ªç¯

```python
# askæ–¹æ³•ä¸­çš„è‡ªåŠ¨è®­ç»ƒé€»è¾‘
def ask(self, question: str, auto_train=True):
    sql = self.generate_sql(question)
    df = self.run_sql(sql)
    
    # æˆåŠŸæ‰§è¡Œçš„æŸ¥è¯¢è‡ªåŠ¨åŠ å…¥è®­ç»ƒé›†
    if len(df) > 0 and auto_train:
        self.add_question_sql(question=question, sql=sql)
        
    return sql, df, visualization
```

---

## 7. æŠ€æœ¯åˆ›æ–°ç‚¹

### 7.1 æ ¸å¿ƒåˆ›æ–°æ¶æ„

```mermaid
graph TB
    subgraph "Vannaæ ¸å¿ƒåˆ›æ–°"
        A[æ¨¡å—åŒ–å¤šé‡ç»§æ‰¿<br/>LLM+å‘é‡DBè§£è€¦] --> B[æŠ€æœ¯ä¼˜åŠ¿]
        C[ä¸‰ç»´RAGæ£€ç´¢<br/>Question+DDL+Doc] --> B
        D[ç¡®å®šæ€§æ•°æ®ç®¡ç†<br/>UUIDå»é‡æœºåˆ¶] --> B
        E[è‡ªåŠ¨è®­ç»ƒåé¦ˆ<br/>å¢é‡å­¦ä¹ èƒ½åŠ›] --> B
        F[æ™ºèƒ½Promptå·¥ç¨‹<br/>åŠ¨æ€ä¸Šä¸‹æ–‡ç®¡ç†] --> B
    end
    
    subgraph "åˆ›æ–°ä»·å€¼"
        B --> G[é«˜å‡†ç¡®æ€§<br/>å¤šç»´ä¸Šä¸‹æ–‡]
        B --> H[å¼ºæ‰©å±•æ€§<br/>30+é›†æˆ]
        B --> I[è‡ªä¼˜åŒ–<br/>æŒç»­æ”¹è¿›]
        B --> J[ç”Ÿäº§å°±ç»ª<br/>ä¼ä¸šçº§ç‰¹æ€§]
    end
```

### 7.2 RAGæŠ€æœ¯åˆ›æ–°è¯¦è§£

#### 7.2.1 ä¸‰ç»´æ£€ç´¢ç³»ç»Ÿ
- **Question-SQLç»´åº¦**ï¼šé€šè¿‡å‘é‡ç›¸ä¼¼åº¦æ‰¾åˆ°å†å²ç›¸ä¼¼æŸ¥è¯¢
- **DDLç»´åº¦**ï¼šæä¾›å‡†ç¡®çš„è¡¨ç»“æ„å’Œå­—æ®µä¿¡æ¯
- **Documentationç»´åº¦**ï¼šåŒ…å«ä¸šåŠ¡é€»è¾‘å’Œä¸Šä¸‹æ–‡è¯´æ˜

#### 7.2.2 æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†
```python
def add_ddl_to_prompt(self, initial_prompt: str, ddl_list: list, max_tokens: int = 14000):
    """åŠ¨æ€DDLæ·»åŠ ç­–ç•¥"""
    if len(ddl_list) > 0:
        initial_prompt += "\n===Tables \n"
        
        for ddl in ddl_list:
            # Tokenè®¡æ•°å’Œä¼˜å…ˆçº§ç®¡ç†
            if (self.str_to_approx_token_count(initial_prompt) + 
                self.str_to_approx_token_count(ddl) < max_tokens):
                initial_prompt += f"{ddl}\n\n"
    
    return initial_prompt
```

### 7.3 å·¥ç¨‹åŒ–åˆ›æ–°

#### 7.3.1 ç¡®å®šæ€§UUIDç”Ÿæˆ
```python
def deterministic_uuid(data: str) -> str:
    """ç¡®ä¿ç›¸åŒæ•°æ®ç”Ÿæˆç›¸åŒIDï¼Œé¿å…é‡å¤å­˜å‚¨"""
    return str(uuid.uuid5(uuid.NAMESPACE_DNS, data))
```

#### 7.3.2 è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©
```python
def submit_prompt(self, prompt):
    num_tokens = sum(len(msg["content"])/4 for msg in prompt)
    
    # æ ¹æ®tokenæ•°é‡æ™ºèƒ½é€‰æ‹©æ¨¡å‹
    if num_tokens > 3500:
        model = "gpt-3.5-turbo-16k"
    else:
        model = "gpt-3.5-turbo"
```

### 7.4 åˆ›æ–°æŠ€æœ¯å¯¹æ¯”

| ä¼ ç»ŸText-to-SQL | Vannaæ¡†æ¶ |
|----------------|----------|
| å•ä¸€æ¨¡å‹è®­ç»ƒ | æ¨¡å—åŒ–ç»„åˆæ¶æ„ |
| é™æ€æç¤ºè¯ | åŠ¨æ€RAGæ£€ç´¢ |
| å•æ¬¡ç”Ÿæˆ | å¢é‡å­¦ä¹ ä¼˜åŒ– |
| æœ‰é™ä¸Šä¸‹æ–‡ | ä¸‰ç»´ä¸Šä¸‹æ–‡æ£€ç´¢ |
| é»‘ç›’ç³»ç»Ÿ | é€æ˜å¯æ§æµç¨‹ |

---

## 8. æŠ€æœ¯å®ç°æ¡†æ¶æ€»ç»“

### 8.1 æ¶æ„ä¼˜åŠ¿æ€»è§ˆ

```mermaid
mindmap
  root((VannaæŠ€æœ¯ä¼˜åŠ¿))
    æ¶æ„è®¾è®¡
      æ¨¡å—åŒ–è®¾è®¡
      æ ‡å‡†åŒ–æ¥å£
      æ˜“äºæ‰©å±•
      ç»„ä»¶è§£è€¦
    
    RAGæŠ€æœ¯
      å¤šç»´æ£€ç´¢
      æ™ºèƒ½ä¸Šä¸‹æ–‡
      å¢é‡å­¦ä¹ 
      åŠ¨æ€ä¼˜åŒ–
    
    å·¥ç¨‹å®ç°
      ç”Ÿäº§å°±ç»ª
      é”™è¯¯å¤„ç†
      æ—¥å¿—ç³»ç»Ÿ
      é…ç½®ç®¡ç†
    
    ç”Ÿæ€é›†æˆ
      30+LLMæ”¯æŒ
      å¤šç§å‘é‡DB
      ä¸°å¯Œè¿æ¥å™¨
      å¯è§†åŒ–é›†æˆ
```

### 8.2 æ ¸å¿ƒæŠ€æœ¯æ ˆ

#### 8.2.1 LLMç”Ÿæ€æ”¯æŒ
- **OpenAIç³»åˆ—**: GPT-3.5-turbo, GPT-4, text-embedding-ada-002
- **Anthropicç³»åˆ—**: Claude-3 Haiku/Sonnet/Opus
- **Googleç³»åˆ—**: Gemini Pro/Ultra, PaLM
- **å¼€æºæ¨¡å‹**: Ollamaæœ¬åœ°éƒ¨ç½²æ”¯æŒ
- **å…¶ä»–å•†ç”¨**: Azure OpenAI, AWS Bedrockç­‰

#### 8.2.2 å‘é‡æ•°æ®åº“ç”Ÿæ€
```mermaid
graph LR
    subgraph "å‘é‡æ•°æ®åº“é€‰æ‹©æŒ‡å—"
        A[éœ€æ±‚è¯„ä¼°] --> B{éƒ¨ç½²æ–¹å¼?}
        B -->|æœ¬åœ°å¼€å‘| C[ChromaDB<br/>è½»é‡å¿«é€Ÿ]
        B -->|ç”Ÿäº§éƒ¨ç½²| D{æ•°æ®è§„æ¨¡?}
        D -->|å°è§„æ¨¡| E[PGVector<br/>SQLç”Ÿæ€]
        D -->|å¤§è§„æ¨¡| F[Pinecone<br/>äº‘æ‰˜ç®¡]
        D -->|è¶…å¤§è§„æ¨¡| G[Milvus<br/>åˆ†å¸ƒå¼]
        
        C --> H[å¼€å‘åŸå‹]
        E --> I[ä¸­å°ä¼ä¸š]
        F --> J[å¿«é€Ÿæ‰©å±•]
        G --> K[å¤§å‹ä¼ä¸š]
    end
```

### 8.3 é€‚ç”¨åœºæ™¯åˆ†æ

#### 8.3.1 ä¼ä¸šåº”ç”¨åœºæ™¯
- **å•†ä¸šæ™ºèƒ½å¹³å°**: è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ¥å£
- **æ•°æ®åˆ†æå·¥å…·**: é™ä½SQLå­¦ä¹ é—¨æ§›
- **å®¢æœç³»ç»Ÿé›†æˆ**: è‡ªåŠ¨æ•°æ®æŸ¥è¯¢å›ç­”
- **æŠ¥è¡¨ç³»ç»Ÿ**: åŠ¨æ€æŠ¥è¡¨ç”Ÿæˆ

#### 8.3.2 æŠ€æœ¯åœºæ™¯
- **æ•°æ®ç§‘å­¦å·¥ä½œæµ**: å¿«é€Ÿæ•°æ®æ¢ç´¢
- **DevOpsç›‘æ§**: è‡ªç„¶è¯­è¨€æ—¥å¿—æŸ¥è¯¢
- **æ•™è‚²åŸ¹è®­**: SQLå­¦ä¹ è¾…åŠ©å·¥å…·

### 8.4 æ€§èƒ½å’Œæ‰©å±•æ€§

#### 8.4.1 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
```python
# ç¼“å­˜æœºåˆ¶
@lru_cache(maxsize=1000)
def get_similar_question_sql(self, question: str):
    """ç¼“å­˜ç›¸ä¼¼é—®é¢˜æ£€ç´¢ç»“æœ"""
    pass

# æ‰¹é‡å¤„ç†
def train_batch(self, training_data_list):
    """æ‰¹é‡è®­ç»ƒæé«˜æ•ˆç‡"""
    embeddings = self.generate_embeddings_batch(training_data_list)
    self.vector_store.add_batch(training_data_list, embeddings)
```

#### 8.4.2 æ‰©å±•æ€§è®¾è®¡
- **æ°´å¹³æ‰©å±•**: å‘é‡æ•°æ®åº“æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²
- **å‚ç›´æ‰©å±•**: æ”¯æŒGPUåŠ é€Ÿçš„embeddingç”Ÿæˆ
- **åŠŸèƒ½æ‰©å±•**: æ’ä»¶åŒ–æ¶æ„ä¾¿äºæ·»åŠ æ–°åŠŸèƒ½

### 8.5 æœ€ä½³å®è·µå»ºè®®

#### 8.5.1 éƒ¨ç½²å»ºè®®
1. **å¼€å‘ç¯å¢ƒ**: ChromaDB + OpenAI
2. **æµ‹è¯•ç¯å¢ƒ**: PGVector + Claude
3. **ç”Ÿäº§ç¯å¢ƒ**: Pinecone + GPT-4

#### 8.5.2 è®­ç»ƒæ•°æ®ç­–ç•¥
1. **åˆå§‹åŒ–**: ä½¿ç”¨`get_training_plan_*`è‡ªåŠ¨ç”Ÿæˆ
2. **è¿­ä»£ä¼˜åŒ–**: åŸºäºç”¨æˆ·åé¦ˆæŒç»­æ”¹è¿›
3. **è´¨é‡æ§åˆ¶**: å®šæœŸæ¸…ç†ä½è´¨é‡è®­ç»ƒæ•°æ®

#### 8.5.3 ç›‘æ§å’Œç»´æŠ¤
```python
# æ€§èƒ½ç›‘æ§
def monitor_query_performance(self):
    metrics = {
        'avg_response_time': self.get_avg_response_time(),
        'success_rate': self.get_success_rate(),
        'training_data_size': len(self.get_training_data())
    }
    return metrics

# æ•°æ®è´¨é‡è¯„ä¼°
def evaluate_training_data_quality(self):
    return self.vector_store.quality_metrics()
```

---

## ç»“è®º

Vannaæ¡†æ¶é€šè¿‡åˆ›æ–°çš„RAGæŠ€æœ¯å’Œæ¨¡å—åŒ–æ¶æ„ï¼Œä¸ºtext-to-SQLé¢†åŸŸæä¾›äº†ä¸€ä¸ªæˆç†Ÿã€å¯æ‰©å±•ã€ç”Ÿäº§å°±ç»ªçš„è§£å†³æ–¹æ¡ˆã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äºï¼š

1. **æŠ€æœ¯åˆ›æ–°**: ä¸‰ç»´RAGæ£€ç´¢ç³»ç»Ÿæ˜¾è‘—æå‡å‡†ç¡®æ€§
2. **æ¶æ„ä¼˜é›…**: æ¨¡å—åŒ–è®¾è®¡å®ç°äº†æå¼ºçš„æ‰©å±•æ€§
3. **å·¥ç¨‹æˆç†Ÿ**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œç”Ÿäº§ç‰¹æ€§
4. **ç”Ÿæ€ä¸°å¯Œ**: æ”¯æŒ30+ç§LLMå’Œå‘é‡æ•°æ®åº“ç»„åˆ

è¯¥æ¡†æ¶ç‰¹åˆ«é€‚åˆä¼ä¸šçº§æ•°æ®åˆ†æã€BIç³»ç»Ÿé›†æˆå’Œæ•™è‚²åŸ¹è®­åœºæ™¯ï¼Œä¸ºä¼ ç»Ÿçš„æ•°æ®æŸ¥è¯¢å·¥ä½œæµå¸¦æ¥äº†é©å‘½æ€§çš„æ”¹è¿›ã€‚éšç€RAGæŠ€æœ¯çš„æŒç»­å‘å±•ï¼ŒVannaæœ‰æœ›åœ¨text-to-SQLé¢†åŸŸç»§ç»­å¼•é¢†æŠ€æœ¯åˆ›æ–°ã€‚